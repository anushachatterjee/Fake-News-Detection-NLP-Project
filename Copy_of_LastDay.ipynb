{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of LastDay.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushachatterjee/Fake-News-Detection-NLP-Project/blob/main/Copy_of_LastDay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7V3xDHFW508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d9f43db2-34d3-4723-e7d8-2a8279fd05eb"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jul  2 16:50:39 2019\n",
        "@author: Nobline\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load training data\n",
        "bodies = pd.read_csv(\"https://raw.githubusercontent.com/Ethan-Tseng/AI4ALL-NLP/master/Week_2/train_bodies.csv\")\n",
        "headlines = pd.read_csv(\"https://raw.githubusercontent.com/Ethan-Tseng/AI4ALL-NLP/master/Week_2/train_stances.csv\")\n",
        "\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def clean(doc):\n",
        "    for i in range(doc.shape[0]):\n",
        "        #lowercasing\n",
        "        doc.at[i] = doc.iloc[i].lower()\n",
        "    print(\"LOWERCASE\")\n",
        "    #print(doc)\n",
        "    for i in range(doc.shape[0]):\n",
        "        #remove punctuation\n",
        "        doc.at[i] = re.sub(r'([^\\s\\w])+', '', doc.iloc[i])\n",
        "    print(\"REMOVE PUNCT\")\n",
        "    #print(doc)\n",
        "    for i in range(doc.shape[0]):\n",
        "        #remove stopwords\n",
        "        doc.at[i] = remove_stopwords(doc.iloc[i])\n",
        "    print(\"REMOVE STOPWORDS\")\n",
        "    #print(doc)\n",
        "    for i in range(doc.shape[0]):\n",
        "        #tokenize\n",
        "        doc.at[i] = word_tokenize(doc.iloc[i])\n",
        "    print(\"TOKENIZE\")\n",
        "    #print(doc)\n",
        "    for i in range(doc.shape[0]):\n",
        "        #lemmatize\n",
        "        for j in range(len(doc.iloc[i])):\n",
        "            doc.iloc[i][j] = lemmatizer.lemmatize(doc.iloc[i][j])\n",
        "    print(\"LEMMATIZE\")\n",
        "    #print(doc)\n",
        "\n",
        "testBodies = pd.read_csv(\"https://raw.githubusercontent.com/Ethan-Tseng/AI4ALL-NLP/master/Week_1/data/competition_test_bodies.csv\")\n",
        "testHeadlines = pd.read_csv(\"https://raw.githubusercontent.com/Ethan-Tseng/AI4ALL-NLP/master/Week_1/data/competition_test_stances.csv\")\n",
        "\n",
        "clean(bodies['articleBody'])\n",
        "clean(headlines['Headline'])\n",
        "clean(testBodies['articleBody'])\n",
        "clean(testHeadlines['Headline'])\n",
        "\n",
        "train_data = pd.merge(bodies, headlines, on='Body ID')\n",
        "test_data = pd.merge(testBodies, testHeadlines, on='Body ID')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "LOWERCASE\n",
            "REMOVE PUNCT\n",
            "REMOVE STOPWORDS\n",
            "TOKENIZE\n",
            "LEMMATIZE\n",
            "LOWERCASE\n",
            "REMOVE PUNCT\n",
            "REMOVE STOPWORDS\n",
            "TOKENIZE\n",
            "LEMMATIZE\n",
            "LOWERCASE\n",
            "REMOVE PUNCT\n",
            "REMOVE STOPWORDS\n",
            "TOKENIZE\n",
            "LEMMATIZE\n",
            "LOWERCASE\n",
            "REMOVE PUNCT\n",
            "REMOVE STOPWORDS\n",
            "TOKENIZE\n",
            "LEMMATIZE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzkqDJFFoFPa"
      },
      "source": [
        "#train_data['combine'] = train_data['articleBody'] + train_data['Headline']\n",
        "combined = pd.concat([train_data['articleBody'],train_data['Headline']])\n",
        "testCombined = pd.concat([test_data['articleBody'],test_data['Headline']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e47w7yrdE8e"
      },
      "source": [
        "\n",
        "#extract tfidf vectors of article body and headline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def default(doc):\n",
        "    return doc\n",
        "\n",
        "def tfidf_extractor(c,b,h):\n",
        "    global vect\n",
        "    vect = TfidfVectorizer(analyzer = 'word', preprocessor = default, tokenizer=default, token_pattern = None)\n",
        "\n",
        "    global bodyTFIDF\n",
        "    global headlineTFIDF\n",
        "    global combineTFIDF\n",
        "    combineTFIDF = vect.fit(c)\n",
        "    bodyTFIDF = vect.transform(b)\n",
        "    headlineTFIDF = vect.transform(h)\n",
        "    \n",
        "\n",
        "    \n",
        "#main action!\n",
        "tfidf_extractor(combined, train_data['articleBody'], train_data['Headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhZrti_U5HsE"
      },
      "source": [
        "def test_tfidf_extractor(b,h):\n",
        "    global testBodyTFIDF\n",
        "    global testHeadlineTFIDF\n",
        "    testBodyTFIDF = vect.transform(b)\n",
        "    testHeadlineTFIDF = vect.transform(h)\n",
        "    \n",
        "\n",
        "    \n",
        "#main action!\n",
        "test_tfidf_extractor(test_data['articleBody'], test_data['Headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEqHc5wmjlA"
      },
      "source": [
        "#COMBINE VECTOR\n",
        "from scipy.sparse import coo_matrix, hstack\n",
        "\n",
        "def stack_headline_body(matrix):\n",
        "  return hstack(matrix)\n",
        "  #print(stackedVector.shape)\n",
        "\n",
        "stackedVector = stack_headline_body([bodyTFIDF, headlineTFIDF])\n",
        "testStackedVector = stack_headline_body([testBodyTFIDF, testHeadlineTFIDF])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxoK-W7G-ZaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c013b1aa-5e58-4a0f-bb44-df261a17b86e"
      },
      "source": [
        "#Store Labels\n",
        "# 0 = unrelated 1 = agree 2 = disagree 3 = discuss\n",
        "#print(train_data['Stance'])\n",
        "train_data[\"labels\"] = ''\n",
        "test_data[\"labels\"] = ''\n",
        "def makeNumStance(data, stance, newColumn):\n",
        "  for i in range(len(data)): \n",
        "      if stance[i] == \"unrelated\": \n",
        "          newColumn[i] = 0\n",
        "  \n",
        "      elif stance[i] == \"agree\": \n",
        "          newColumn[i] = 1\n",
        "  \n",
        "      elif stance[i] == \"disagree\": \n",
        "          newColumn[i] = 2\n",
        "\n",
        "      elif stance[i] == \"discuss\": \n",
        "          newColumn[i] = 3\n",
        "\n",
        "      else:\n",
        "        newColumn[i] = 'unk'\n",
        "\n",
        "  return newColumn\n",
        "    \n",
        "train_data['labels'] = makeNumStance(train_data, train_data['Stance'], train_data['labels'])\n",
        "test_data['labels'] = makeNumStance(test_data, test_data['Stance'], test_data['labels'])\n",
        "\n",
        "# print(train_data['labels'])\n",
        "# print(test_data['labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrfA5K6VSO6X"
      },
      "source": [
        "# Different way to combine the vectors. If the word is present in headline but not in bodies, then mark it as 1, else mark it as 0\n",
        "# It's okay if this is a little hard to understand, you can assume that this is a blackbox :)\n",
        "\n",
        "import scipy\n",
        "\n",
        "def convert_data_to_type_2(bodies, headlines):\n",
        "    coo_headlines = headlines.tocoo()    \n",
        "    for i,j,v in zip(coo_headlines.row, coo_headlines.col, coo_headlines.data):\n",
        "        if bodies[i,j] == 0:\n",
        "            headlines[i,j] = 1\n",
        "        else:\n",
        "            headlines[i,j] = 0\n",
        "    return headlines\n",
        "\n",
        "train_data_type2 = convert_data_to_type_2(bodyTFIDF, headlineTFIDF)\n",
        "test_data_type2 = convert_data_to_type_2(testBodyTFIDF, testHeadlineTFIDF)\n",
        "\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import Normalizer\n",
        "DataNormalizer = Normalizer()\n",
        "DataNormalizer.fit(train_data_type2)\n",
        "train_data_type2 = DataNormalizer.transform(train_data_type2)\n",
        "test_data_type2 = DataNormalizer.transform(test_data_type2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GGlquU_QGJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f94fc7b2-f5f9-4637-93aa-ef9be3192d55"
      },
      "source": [
        "# Import packages to copy data\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "# Define variables which are easy to understand\n",
        "train_data_type1 = copy.deepcopy(stackedVector)\n",
        "test_data_type1 = copy.deepcopy(testStackedVector)\n",
        "\n",
        "train_data_type2 = copy.deepcopy(train_data_type2)\n",
        "test_data_type2 = copy.deepcopy(test_data_type2)\n",
        "\n",
        "# Define the labels\n",
        "train_labels = np.array(train_data['labels'], dtype=np.int)\n",
        "test_labels = np.array(test_data['labels'], dtype=np.int)\n",
        "\n",
        "# Print the shapes of data\n",
        "print(\"Shape of train data where we combine vectors is: {}\".format(train_data_type1.shape))\n",
        "print(\"Shape of test data where we combine vectors is: {}\".format(test_data_type1.shape))\n",
        "\n",
        "print(\"Shape of train data where we \\\"subtract\\\" vectors is: {}\".format(train_data_type2.shape))\n",
        "print(\"Shape of test data where we \\\"subtract\\\" vectors is: {}\".format(test_data_type2.shape))\n",
        "\n",
        "# Print the shapes of the labels\n",
        "print(\"Shape of train labels is: {}\".format(train_labels.shape))\n",
        "print(\"Shape of test labels is: {}\".format(test_labels.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data where we combine vectors is: (49972, 46822)\n",
            "Shape of test data where we combine vectors is: (25413, 46822)\n",
            "Shape of train data where we \"subtract\" vectors is: (49972, 23411)\n",
            "Shape of test data where we \"subtract\" vectors is: (25413, 23411)\n",
            "Shape of train labels is: (49972,)\n",
            "Shape of test labels is: (25413,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpt-nsakrrdD"
      },
      "source": [
        "# Split the training data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the train data you want to use\n",
        "train_data_selected = train_data_type2\n",
        "test_data_selected = test_data_type2\n",
        "\n",
        "from scipy.sparse import vstack\n",
        "train_data_stacked = vstack([train_data_selected, test_data_selected])\n",
        "train_labels_stacked = np.hstack([train_labels, test_labels])\n",
        "\n",
        "# Define the test data\n",
        "train_split, test_split, train_labels_split, test_labels_split = train_test_split(train_data_stacked, train_labels_stacked, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKEGxXL4VEWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "c396b5fc-9e86-4f3d-db8d-2e0b529cd80c"
      },
      "source": [
        "# Compute accuracy using Decision Trees, Nearest Neighbors, SVM, Neural Networks\n",
        "\n",
        "# Define all the import statements\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Import decision tree\n",
        "# Import SVM\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Import the evaluation metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the classifier\n",
        "classifier = KNeighborsClassifier()\n",
        "# classifier = MLPClassifier(verbose=True, max_iter=40, early_stopping=True)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(train_split, train_labels_split)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "print(\"The performance is:\")\n",
        "print(classification_report(test_labels_split, classifier.predict(test_split)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The performance is:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f9db023d5d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Calculate the accuracy of the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The performance is:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 **kwds))\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ball_tree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kd_tree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0;32m-> 1595\u001b[0;31m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[1;32m   1596\u001b[0m         if ((X is Y or Y is None)\n\u001b[1;32m   1597\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    562\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    563\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    524\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m            \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m            indptr, indices, data)\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD8aB5zbuWju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "300043c0-dc7a-4337-b112-56a390f4180d"
      },
      "source": [
        "# # Compute accuracy using Decision Trees, Nearest Neighbors, SVM, Neural Networks\n",
        "\n",
        "# # Define all the import statements\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# # Import decision tree\n",
        "# # Import SVM\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# # Import the evaluation metrics\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# # Define the classifier\n",
        "# classifier = KNeighborsClassifier()\n",
        "# # classifier = MLPClassifier(verbose=True, max_iter=40, early_stopping=True)\n",
        "classifier = tree.DecisionTreeClassifier(min_samples_leaf=1)\n",
        "\n",
        "# # Train the classifier\n",
        "classifier.fit(train_data_selected, train_labels)\n",
        "\n",
        "\n",
        "# # Calculate the accuracy of the classifier\n",
        "print(\"The performance is:\")\n",
        "print(classification_report(test_labels, classifier.predict(test_data_type2)))\n",
        "\n",
        "# # Calculate the accuracy of the classifier\n",
        "# print(\"The performance is:\")\n",
        "# print(classification_report(train_labels, classifier.predict(train_data_type1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The performance is:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.56      0.70     18349\n",
            "           1       0.14      0.26      0.18      1903\n",
            "           2       0.02      0.02      0.02       697\n",
            "           3       0.31      0.71      0.43      4464\n",
            "\n",
            "    accuracy                           0.55     25413\n",
            "   macro avg       0.35      0.39      0.33     25413\n",
            "weighted avg       0.75      0.55      0.60     25413\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}